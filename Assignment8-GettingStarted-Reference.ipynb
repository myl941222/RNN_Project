{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Machine Learning\n",
    "### Assignment 8 - Language Modeling With an Recurrent Neural Networks (RNN)\n",
    "\n",
    "Description:\n",
    "This assignment involves working with language models developed with pretrained word vectors. We use sentences (sequences of words) to train language models for predicting movie review sentiment (thumbs up versus thumbs down). We study effects of word vector size, vocabulary size, and neural network structure (hyperparameters) on classification performance. We build on resources for recurrent neural networks (RNNs) as implemented in TensorFlow. RNNs are well suited to the analysis of sequences, as needed for natural language processing (NLP). \n",
    "\n",
    "Initial background reading for this assignment comes from Chapter 14 (pp. 379–411) of the Géron textbook:\n",
    "Géron, A. (2017). Hands-on machine learning with Scikit-Learn & TensorFlow: Concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O’Reilly. [ISBN-13 978-1-491-96229-9]. \n",
    "\n",
    "Specialized RNN models have been developed to accommodate the needs of many language processing tasks. Larger relevant vocabularies are usually associated with more accurate models, but training with larger vocabularies requires more memory and longer processing times. We can speed up the training process by using pretrained word vectors and subsets of pretrained word vectors.\n",
    "\n",
    "Technologies such as word2vec, GloVe (global vectors), and fastText provide ways of representing words as numeric vectors. These numeric vectors or neural network embeddings capture the meaning of words as well as their common usage as parts of speech. Word embeddings have extensive applications in natural language processing.\n",
    "\n",
    "This assignment requires the use of two pretrained word embeddings selected from a list of supported vectors. That is, we replace each word in a sentence or sequence with a vector of numbers. Methods for downloading word embeddings are provided in the Python package chakin { https://github.com/chakki-works/chakin } \n",
    "\n",
    "Early work on word2vec embeddings is cited in these references:\n",
    "- Mikolov, T., et al. (2013). Efficient Estimation of Word Representations in Vector Space\n",
    "https://arxiv.org/pdf/1301.3781.pdf \n",
    "\n",
    "- Mikolov, T., et al. (2013). Linguistic Regularities in Continuous Space Word Representations (Links to an external site.)Links to an external site.\n",
    "https://www.aclweb.org/anthology/N13-1090 \n",
    "\n",
    "GloVe, a method for estimating pretrained word vectors, was developed at Stanford \n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "A tutorial and code for using GloVe embeddings is available here. \n",
    "https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "\n",
    "A third set of pretrained vectors is fastText, described here \n",
    "https://arxiv.org/pdf/1607.04606.pdf\n",
    "\n",
    "Word embeddings are an active area of research, as shown in recent developments in probabilistic fastText \n",
    "http://aclweb.org/anthology/P18-1001\n",
    "\n",
    "We can also test vocabulary sizes associated with pretrained word vectors, defined as part of the language model. For example, we might compare a vocabulary of the top 10,000 words in English versus a vocabulary of the top 30,000 words.\n",
    "\n",
    "(Optional) We can test alternative RNN structures and settings of hyperparameters.\n",
    "\n",
    "Assignment Requirements\n",
    "- Install the Python chakin package, obtain GloVe (and perhaps non-GloVe) embeddings.\n",
    "- Load and run jump-start code for the assignment, which uses pretrained word vectors from GloVe.6B.50d, a vocabulary of 10,000 words, and movie review data. \n",
    "- Revise the jump-start code to accommodate two pretrained word vectors and two vocabulary sizes. These represent the cells of a completely crossed 2-by-2 experimental design, defining four distinct language models.\n",
    "- (Optional) Test two or more alternative RNN structures or hyperparameter settings.\n",
    "- Build and evaluate at least four language models of the experimental design. For each cell in the design, compute classification accuracy in the test set.\n",
    "- Evaluate the four language models and make recommendations to management.\n",
    "\n",
    "Management Problem\n",
    "- Suppose management is thinking about using a language model to classify written customer reviews and call and complaint logs. If the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers.\n",
    "- How would you advise senior management? What kinds of systems and methods would be most relevant to the customer services function? Considering the results of this assignment in particular, what is needed to make an automated customer support system that is capable of identifying negative customer feelings? What can data scientists do to make language models more useful in a customer service function?\n",
    "\n",
    "Grading Guidelines (50 points)\n",
    "(1) Data preparation, exploration, visualization (10 points)\n",
    "(2) Review research design and modeling methods (10 points)\n",
    "(3) Review results, evaluate models (10 points)\n",
    "(4) Implementation and programming (10 points)\n",
    "(5) Exposition, problem description, and management recommendations (10 points)\n",
    "\n",
    "Deliverables and File Formats\n",
    "Create a folder or directory with all supplementary files with your last name at the beginning of the folder name, compress that folder with zip compression, and post the zip-archived folder under the assignment link in Canvas. The following files should be included in an archive folder/directory that is uploaded as a single zip-compressed file. (Use zip, not StuffIt or any 7z or other compression method.)\n",
    "\n",
    "Provide a double-spaced paper with a two-page maximum for the text. The paper should include (1) a summary and problem definition for management; (2) discussion of the research design, measurement and statistical methods, traditional and machine learning methods employed; (3) overview of programming work; and (4) review of results with recommendations for management. (The paper must be provided as an Adobe Acrobat pdf file. MS Word files are NOT acceptable.)\n",
    "\n",
    "Files or links to files should be provided in the format as used by the Python program.\n",
    "\n",
    "Because only minor revisions to the jump-start code are needed to implement the language models being tested in this assignment, it is not necessary to provide Python code for the language models.\n",
    "\n",
    "Output from the program, such as console listing/logs, text files, and graphics output for visualizations.\n",
    "List file names and descriptions of files in the zip-compressed folder/directory.\n",
    "\n",
    "Appendix: chakin-supported vectors\n",
    "Here are a few of the chakin-supported pretrained word vectors as of August 12, 2018:\n",
    "[index number] Name\tDimension\tCorpus\tVocab Size\tMethod\tLanguage\n",
    "[0] fastText(ar)\t300\tWikipedia\t610K\tfastText\tArabic\n",
    "[1] fastText(de)\t300\tWikipedia\t2.3M\tfastText\tGerman\n",
    "[2] fastText(en)\t300\tWikipedia\t2.5M\tfastText\tEnglish\n",
    "[3] fastText(es)\t300\tWikipedia\t985K\tfastText\tSpanish\n",
    "[4] fastText(fr)\t300\tWikipedia\t1.2M\tfastText\tFrench\n",
    "[5] fastText(it)\t300\tWikipedia\t871K\tfastText\tItalian\n",
    "[6] fastText(ja)\t300\tWikipedia\t580K\tfastText\tJapanese\n",
    "[7] fastText(ko)\t300\tWikipedia\t880K\tfastText\tKorean\n",
    "[8] fastText(pt)\t300\tWikipedia\t592K\tfastText\tPortuguese\n",
    "[9] fastText(ru)\t300\tWikipedia\t1.9M\tfastText\tRussian\n",
    "[10] fastText(zh)\t300\tWikipedia\t330K\tfastText\tChinese\n",
    "[11] GloVe.6B.50d\t50\tWikipedia+Gigaword 5 (6B)\t400K\tGloVe\tEnglish\n",
    "[12] GloVe.6B.100d\t100\tWikipedia+Gigaword 5 (6B)\t400K\tGloVe\tEnglish\n",
    "[13] GloVe.6B.200d\t200\tWikipedia+Gigaword 5 (6B)\t400K\tGloVe\tEnglish\n",
    "[14] GloVe.6B.300d\t300\tWikipedia+Gigaword 5 (6B)\t400K\tGloVe\tEnglish\n",
    "[15] GloVe.42B.300d\t300\tCommon Crawl(42B)\t1.9M\tGloVe\tEnglish\n",
    "[16] GloVe.840B.300d\t300\tCommon Crawl(840B)\t2.2M\tGloVe\tEnglish\n",
    "[17] GloVe.Twitter.25d\t25\tTwitter(27B)\t1.2M\tGloVe\tEnglish\n",
    "[18] GloVe.Twitter.50d\t50\tTwitter(27B)\t1.2M\tGloVe\tEnglish\n",
    "[19] GloVe.Twitter.100d\t100\tTwitter(27B)\t1.2M\tGloVe\tEnglish\n",
    "[20] GloVe.Twitter.200d\t200\tTwitter(27B)\t1.2M\tGloVe\tEnglish\n",
    "[21] word2vec.GoogleNews\t300\tGoogle News(100B)\t3.0M\tword2vec\tEnglish\n",
    "[22] word2vec.Wiki-NEologd.50d\t50\tWikipedia\t335K\tword2vec + NEologd\tJapanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#ignore tensorflow related warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collection\n",
    "##### RNN - Movie Review Sentiment  PART 1 : run chakin to get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports for our work\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd  # data frame operations  \n",
    "import numpy as np  # arrays and math functions\n",
    "import matplotlib.pyplot as plt  # static plotting\n",
    "import re # regular expressions\n",
    "import scipy\n",
    "import os # Operation System\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "#from __future__ import division, print_function, unicode_literals\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Python chakin package previously installed by \n",
    "#    pip install chakin\n",
    "import chakin  \n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search(lang='English')  # lists available indices in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify English embeddings file to download and install by index number, number of dimensions, and subfoder name\n",
    "# Note that GloVe 50-, 100-, 200-, and 300-dimensional folders are downloaded with a single zip download\n",
    "CHAKIN_INDEX = 11\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"gloVe.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n",
      "Embeddings already extracted.\n",
      "\n",
      "Run complete\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0:\n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')\n",
    "\n",
    "# After this step there should be\n",
    "# embeddings folder in the current working directory A\n",
    "# Directory called glove.6b within embeddings directory\n",
    "# 4 files within it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Preparation & Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports for our work\n",
    "from __future__ import absolute_import # from __future__ import absolute_import\n",
    "from __future__ import division # Changing the Division Operator\n",
    "from __future__ import print_function #Make print a function\n",
    "import numpy as np # import numpy\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "import re  # regular expressions\n",
    "from collections import defaultdict # dict subclass that calls a factory function to supply missing values\n",
    "import nltk # Natural Language Toolkit\n",
    "from nltk.tokenize import TreebankWordTokenizer #tokenize text\n",
    "import tensorflow as tf #TensorFlow\n",
    "import time # Record processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value for random number generators to obtain reproducible results\n",
    "RANDOM_SEED = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\myl94\\\\OneDrive\\\\Desktop\\\\ds projects\\\\422 exercises\\\\Exercises 8\\\\Assignments'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current working directory\n",
    "os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "#     tf.compat.v1.get_default_graph()\n",
    "#     tf.random.set_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_STOPWORDS = False  # no stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "embeddings_directory = 'embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for loading embeddings follows methods described in\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "# for the requested pre-trained word embeddings\n",
    "\n",
    "# Note the use of defaultdict data structure from the Python Standard Library\n",
    "# collections_defaultdict.py lets the caller specify a default value up front\n",
    "# The default value will be retuned if the key is not a known dictionary key\n",
    "# That is, unknown words are represented by a vector of zeros\n",
    "# For word embeddings, this default value is a vector of zeros\n",
    "# Documentation for the Python standard library:\n",
    "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
    "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
    "\n",
    "\n",
    "# Load a embedding text file\n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, we return a tuple of two dictionaries \n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from embeddings/gloVe.6B\\glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "# Check if the loaded embedding files glove.6B.50d. successfully.   \"glove.6B.50d.\"was ontained though \"run-chakin-to-get-embeddings-v001.py\"\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "# Note: unknown words have representations with values [0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "# Additional background code from\n",
    "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
    "# shows the general structure of the data structures for word embeddings\n",
    "# This code is modified for our purposes in language modeling \n",
    "\n",
    "# Check vocabrary size and embedding dimention\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "#Check embedding data\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# code for working with movie reviews data \n",
    "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
    "#    Upper Saddle River, N.J.: Pearson Education.\n",
    "#    ISBN-13: 978-0-13-388644-3\n",
    "# This original study used a simple bag-of-words approach\n",
    "# to sentiment analysis, along with pre-defined lists of\n",
    "# negative and positive words.        \n",
    "# Code available at:  https://github.com/mtpa/wnds       \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text parsing function for creating text documents \n",
    "# there is more we could do for data preparation \n",
    "# stemming... looking for contractions... possessives... \n",
    "# but we will work with what we have in this parsing function\n",
    "# if we want to do stemming at a later time, we can use\n",
    "#     porter = nltk.PorterStemmer()  \n",
    "# in a construction like this\n",
    "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\myl94\\\\OneDrive\\\\Desktop\\\\ds projects\\\\422 exercises\\\\Exercises 8\\\\Assignments'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: C:\\Users\\myl94\\OneDrive\\Desktop\\ds projects\\422 exercises\\Exercises 8\\Assignments./train/movie-reviews-negative\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Download and install both negtive and positive movies reviews from the Technology Resources Section \n",
    "# within the Jump Start Program for Assignment 8 Module. This entails saving the 'movie-reviews-negative'\n",
    "# and 'movie-reviews-positive' directories from the run-jump-start-rnn-sentiment-v002.zip \n",
    "# or  run-jump-start-rnn-sentiment-big-v002.zip file to your working directory\n",
    "# -----------------------------------------------\n",
    "\n",
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Set path to the negative word dictionary, \"moive-reviews-negative\"\n",
    "dir_name = os.getcwd() + './train/movie-reviews-negative'\n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under C:\\Users\\myl94\\OneDrive\\Desktop\\ds projects\\422 exercises\\Exercises 8\\Assignments./train/movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "    return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: ./train/movie-reviews-positive\n",
      "500 files found\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Set path to the positive word dictionary, \"moive-reviews-positive\"\n",
    "dir_name = './train/movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document files under ./train/movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        data = tf.compat.as_str(f.read())\n",
    "        data = data.lower()\n",
    "        data = text_parse(data)\n",
    "        data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "    return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "        embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: story\n",
      "Embedding for this word:\n",
      " [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\n",
      " -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\n",
      " -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\n",
      " -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\n",
      "  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\n",
      "  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\n",
      " -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\n",
      " -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\n",
      "  0.23625    0.26451  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.48251    0.87746   -0.23455    0.0262     0.79691    0.43102\n",
      " -0.60902   -0.60764   -0.42812   -0.012523  -1.2894     0.52656\n",
      " -0.82763    0.30689    1.1972    -0.47674   -0.46885   -0.19524\n",
      " -0.28403    0.35237    0.45536    0.76853    0.0062157  0.55421\n",
      "  1.0006    -1.3973    -1.6894     0.30003    0.60678   -0.46044\n",
      "  2.5961    -1.2178     0.28747   -0.46175   -0.25943    0.38209\n",
      " -0.28312   -0.47642   -0.059444  -0.59202    0.25613    0.21306\n",
      " -0.016129  -0.29873   -0.19468    0.53611    0.75459   -0.4112\n",
      "  0.23625    0.26451  ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[0][0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: but\n",
      "Embedding for this word:\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n"
     ]
    }
   ],
   "source": [
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[6][9][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: from\n",
      "Embedding for this word:\n",
      " [ 0.41037   0.11342   0.051524 -0.53833  -0.12913   0.22247  -0.9494\n",
      " -0.18963  -0.36623  -0.067011  0.19356  -0.33044   0.11615  -0.58585\n",
      "  0.36106   0.12555  -0.3581   -0.023201 -1.2319    0.23383   0.71256\n",
      "  0.14824   0.50874  -0.12313  -0.20353  -1.82      0.22291   0.020291\n",
      " -0.081743 -0.27481   3.7343   -0.01874  -0.084522 -0.30364   0.27959\n",
      "  0.043328 -0.24621   0.015373  0.49751   0.15108  -0.01619   0.40132\n",
      "  0.23067  -0.10743  -0.36625  -0.051135  0.041474 -0.36064  -0.19616\n",
      " -0.81066 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.41037   0.11342   0.051524 -0.53833  -0.12913   0.22247  -0.9494\n",
      " -0.18963  -0.36623  -0.067011  0.19356  -0.33044   0.11615  -0.58585\n",
      "  0.36106   0.12555  -0.3581   -0.023201 -1.2319    0.23383   0.71256\n",
      "  0.14824   0.50874  -0.12313  -0.20353  -1.82      0.22291   0.020291\n",
      " -0.081743 -0.27481   3.7343   -0.01874  -0.084522 -0.30364   0.27959\n",
      "  0.043328 -0.24621   0.015373  0.49751   0.15108  -0.01619   0.40132\n",
      "  0.23067  -0.10743  -0.36625  -0.051135  0.041474 -0.36064  -0.19616\n",
      " -0.81066 ]\n"
     ]
    }
   ],
   "source": [
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings[999][39][:])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Apply embeddings to numpy\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit Learn for random splitting of the data  \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training data:  (800, 40, 50)\n",
      "Shape of Test data:  (200, 40, 50)\n",
      "\n",
      "Shape of Training data:  (800,)\n",
      "Shape of Test data:  (200,)\n"
     ]
    }
   ],
   "source": [
    "# Set training and test data\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "print(\"Shape of Training data: \", X_train.shape)\n",
    "print(\"Shape of Test data: \", X_test.shape)\n",
    "\n",
    "print(\"\\nShape of Training data: \", y_train.shape)\n",
    "print(\"Shape of Test data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model: Simple RNN with BPTT\n",
    "##### GloVe.6B.50d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------      \n",
    "# We use a very simple Recurrent Neural Network for this assignment\n",
    "# Géron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
    "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
    "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
    "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
    "#    Source code available at https://github.com/ageron/handson-ml\n",
    "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
    "#    See section on Training an sequence Classifier, # In [34]:\n",
    "#    which uses the MNIST case data...  we revise to accommodate\n",
    "#    the movie review data in this assignment    \n",
    "# --------------------------------------------------------------------------  \n",
    "\n",
    "#Training base model by using RNN backpropagation through time (BPTT).\n",
    "reset_graph() # Refresh graph to make output stable across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Consutruction Phase ###\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001 # Learning rate = 0.001\n",
    "\n",
    "#Set placeholder for X and Y\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.compat.v1.placeholder(tf.int32, [None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-42-f9538ea9609b>:2: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-42-f9538ea9609b>:3: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x000001E1DE813C48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x000001E1DE813C48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x000001E1DE813C48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x000001E1DE813C48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-42-f9538ea9609b>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001E1DE83B948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001E1DE83B948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001E1DE83B948>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001E1DE83B948>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "#Set basic cell and dynamic unrolling through time\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "# Compute logits and cross entropy for cost function\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "# Set Cost function\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "# Training with AdamOptimizaer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Evaluation       \n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution Phase###\n",
    "\n",
    "# Set number of epochs and batch size for training model.\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Record start time for neural network training\n",
    "start_time_base = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.48 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.505\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.51 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training model by RNN\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train_base = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test_base = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train_base, 'Test accuracy:', acc_test_base)\n",
    "        \n",
    "        save_path = saver.save(sess, \"./my_catdog_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./my_catdog_model\n"
     ]
    }
   ],
   "source": [
    "# Make prediction by using test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_catdog_model\") # or better, use save_path\n",
    "    X_new_scaled = X_test[:50]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred_base = np.argmax(Z, axis=1)\n",
    "    accuracy_base = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    accuracy_base_y = accuracy.eval(feed_dict={X: X_train, y: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Base Model --------\n",
      "RNN backpropagation through time (BPTT)\n",
      "\n",
      "Predicted classes: [0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 1 0 1 0 0 0 1 0 1 0]\n",
      "Actual classes: [0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0]\n",
      "Test Set Accuracy: 0.68\n",
      "Train Set Accuracy: 0.77625\n",
      "\n",
      "Start time: 21.982894\n",
      "Stop time: 40.0879267\n",
      "processing time: 18.105032699999995\n"
     ]
    }
   ],
   "source": [
    "# Print prediction classes and actual classes.\n",
    "print(\"-------- Base Model --------\")\n",
    "print(\"RNN backpropagation through time (BPTT)\")\n",
    "print(\"\\nPredicted classes:\", y_pred_base)\n",
    "print(\"Actual classes:\", y_test[:25])\n",
    "print(\"Test Set Accuracy:\", accuracy_base)\n",
    "print(\"Train Set Accuracy:\", accuracy_base_y)\n",
    "# Record end time for neural network training\n",
    "stop_time_base = time.clock()\n",
    "\n",
    "#Total processing time\n",
    "runtime_base = stop_time_base - start_time_base \n",
    "\n",
    "print(\"\\nStart time:\", start_time_base)\n",
    "print(\"Stop time:\", stop_time_base)\n",
    "print(\"processing time:\", runtime_base)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: RNN with LSTM cells and 3 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Add\n",
    "from keras.layers import Input, LSTM, GRU\n",
    "from keras.models import Sequential, Model\n",
    "def generateModel1():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model\n",
    "start_time_base_1 = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 40, 32)            10624     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 27,297\n",
      "Trainable params: 27,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = generateModel1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\myl94\\anaconda3\\envs\\jeff\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.3076 - accuracy: 0.5125 - val_loss: 0.2590 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 691us/step - loss: 0.2506 - accuracy: 0.5387 - val_loss: 0.2604 - val_accuracy: 0.4750\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 0s 612us/step - loss: 0.2428 - accuracy: 0.5775 - val_loss: 0.2367 - val_accuracy: 0.6350\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 634us/step - loss: 0.2288 - accuracy: 0.6475 - val_loss: 0.2251 - val_accuracy: 0.6300\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 630us/step - loss: 0.2242 - accuracy: 0.6275 - val_loss: 0.2169 - val_accuracy: 0.6200\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 676us/step - loss: 0.2102 - accuracy: 0.6712 - val_loss: 0.2212 - val_accuracy: 0.6750\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 691us/step - loss: 0.2028 - accuracy: 0.6737 - val_loss: 0.2338 - val_accuracy: 0.6600\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 643us/step - loss: 0.1958 - accuracy: 0.6925 - val_loss: 0.2172 - val_accuracy: 0.6550\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 649us/step - loss: 0.1885 - accuracy: 0.6925 - val_loss: 0.2354 - val_accuracy: 0.6400\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 651us/step - loss: 0.1854 - accuracy: 0.7125 - val_loss: 0.2211 - val_accuracy: 0.6750\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 645us/step - loss: 0.1907 - accuracy: 0.7150 - val_loss: 0.2160 - val_accuracy: 0.6500\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 0s 604us/step - loss: 0.1839 - accuracy: 0.7175 - val_loss: 0.2154 - val_accuracy: 0.6750\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 726us/step - loss: 0.1671 - accuracy: 0.7613 - val_loss: 0.2257 - val_accuracy: 0.6350\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 769us/step - loss: 0.1676 - accuracy: 0.7375 - val_loss: 0.2075 - val_accuracy: 0.6750\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 873us/step - loss: 0.1602 - accuracy: 0.7713 - val_loss: 0.2085 - val_accuracy: 0.6800\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 817us/step - loss: 0.1534 - accuracy: 0.7613 - val_loss: 0.2287 - val_accuracy: 0.6350\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 738us/step - loss: 0.1515 - accuracy: 0.7850 - val_loss: 0.2200 - val_accuracy: 0.6750\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 790us/step - loss: 0.1437 - accuracy: 0.7750 - val_loss: 0.2222 - val_accuracy: 0.6750\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 724us/step - loss: 0.1423 - accuracy: 0.8025 - val_loss: 0.2224 - val_accuracy: 0.6800\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 717us/step - loss: 0.1429 - accuracy: 0.7900 - val_loss: 0.2174 - val_accuracy: 0.6700\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 737us/step - loss: 0.1365 - accuracy: 0.8075 - val_loss: 0.2387 - val_accuracy: 0.6700\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 786us/step - loss: 0.1291 - accuracy: 0.8125 - val_loss: 0.2385 - val_accuracy: 0.6650\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 752us/step - loss: 0.1212 - accuracy: 0.8325 - val_loss: 0.2286 - val_accuracy: 0.6750\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 754us/step - loss: 0.1152 - accuracy: 0.8388 - val_loss: 0.2302 - val_accuracy: 0.6650\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 814us/step - loss: 0.1102 - accuracy: 0.8550 - val_loss: 0.2350 - val_accuracy: 0.6900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 709us/step - loss: 0.1032 - accuracy: 0.8575 - val_loss: 0.2492 - val_accuracy: 0.6800\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 691us/step - loss: 0.1010 - accuracy: 0.8700 - val_loss: 0.2375 - val_accuracy: 0.6950\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 699us/step - loss: 0.0902 - accuracy: 0.8850 - val_loss: 0.2452 - val_accuracy: 0.6950\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 699us/step - loss: 0.0847 - accuracy: 0.8975 - val_loss: 0.2488 - val_accuracy: 0.6850\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 711us/step - loss: 0.0782 - accuracy: 0.9100 - val_loss: 0.2422 - val_accuracy: 0.6850\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 745us/step - loss: 0.0777 - accuracy: 0.9112 - val_loss: 0.2460 - val_accuracy: 0.6900\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 693us/step - loss: 0.0711 - accuracy: 0.9225 - val_loss: 0.2462 - val_accuracy: 0.7000\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 692us/step - loss: 0.0700 - accuracy: 0.9262 - val_loss: 0.2402 - val_accuracy: 0.6800\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 733us/step - loss: 0.0616 - accuracy: 0.9388 - val_loss: 0.2469 - val_accuracy: 0.6850\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 873us/step - loss: 0.0544 - accuracy: 0.9488 - val_loss: 0.2566 - val_accuracy: 0.6800\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 684us/step - loss: 0.0504 - accuracy: 0.9600 - val_loss: 0.2677 - val_accuracy: 0.6600\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 671us/step - loss: 0.0441 - accuracy: 0.9712 - val_loss: 0.2836 - val_accuracy: 0.6750\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 678us/step - loss: 0.0445 - accuracy: 0.9650 - val_loss: 0.2897 - val_accuracy: 0.6700\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 676us/step - loss: 0.0373 - accuracy: 0.9750 - val_loss: 0.2787 - val_accuracy: 0.6600\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 687us/step - loss: 0.0356 - accuracy: 0.9762 - val_loss: 0.2881 - val_accuracy: 0.7150\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 682us/step - loss: 0.0444 - accuracy: 0.9675 - val_loss: 0.2660 - val_accuracy: 0.7150\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.0373 - accuracy: 0.9750 - val_loss: 0.2912 - val_accuracy: 0.6800\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 674us/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.2779 - val_accuracy: 0.6750\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 676us/step - loss: 0.0268 - accuracy: 0.9875 - val_loss: 0.2827 - val_accuracy: 0.6750\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 663us/step - loss: 0.0204 - accuracy: 0.9950 - val_loss: 0.2949 - val_accuracy: 0.6750\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 666us/step - loss: 0.0224 - accuracy: 0.9912 - val_loss: 0.2930 - val_accuracy: 0.6800\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 677us/step - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.2985 - val_accuracy: 0.6800\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 657us/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.2751 - val_accuracy: 0.6850\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 678us/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.2904 - val_accuracy: 0.6750\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 680us/step - loss: 0.0125 - accuracy: 0.9975 - val_loss: 0.2927 - val_accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import adam\n",
    "model.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "count = 0\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing time\n",
      "Test Set Accuracy: 0.9975000023841858\n",
      "Train Set Accuracy: 0.6850000023841858\n",
      "processing time 31.783718400000005\n"
     ]
    }
   ],
   "source": [
    "accuracy_1 = model.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy_base_y_1 = model.evaluate(X_train, y_train,verbose = 0)\n",
    "stop_time_base_1 = time.clock()\n",
    "time_1 = stop_time_base_1 - start_time_base_1\n",
    "print(\"processing time\")\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_1[1])\n",
    "print(\"Train Set Accuracy:\", accuracy_1[1])\n",
    "print(\"processing time\", time_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: RNN with LSTM cells and 3 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVOCABSIZE_1 = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_factory():\n",
    "    return EVOCABSIZE_1  # last/unknown-word row in limited_index_to_embedding\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index_1 = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "limited_index_to_embedding_1 = index_to_embedding[0:EVOCABSIZE_1,:]\n",
    "print(len(limited_index_to_embedding_1))\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding_1 = np.append(limited_index_to_embedding_1, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 30000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE_1, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding_1 = limited_index_to_embedding_1[limited_word_to_index_1[word_]]\n",
    "    print(word_ + \": \", embedding_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1 = []    \n",
    "for doc in documents:\n",
    "    embedding_1 = []\n",
    "    for word in doc:\n",
    "        embedding_1.append(limited_index_to_embedding_1[limited_word_to_index_1[word]]) \n",
    "    embeddings_1.append(embedding_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array_1 = np.array(embeddings_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training data:  (800, 40, 50)\n",
      "Shape of Test data:  (200, 40, 50)\n",
      "\n",
      "Shape of Training data:  (800,)\n",
      "Shape of Test data:  (200,)\n"
     ]
    }
   ],
   "source": [
    "# Set training and test data\n",
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "X1_train, X1_test, y1_train, y1_test = \\\n",
    "    train_test_split(embeddings_array_1, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "\n",
    "print(\"Shape of Training data: \", X1_train.shape)\n",
    "print(\"Shape of Test data: \", X1_test.shape)\n",
    "\n",
    "print(\"\\nShape of Training data: \", y1_train.shape)\n",
    "print(\"Shape of Test data: \", y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_2 = time.clock()\n",
    "def generateModel2():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 40, 32)            10624     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 27,297\n",
      "Trainable params: 27,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = generateModel2()\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2617 - accuracy: 0.5150 - val_loss: 0.2455 - val_accuracy: 0.5250\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 651us/step - loss: 0.2420 - accuracy: 0.5788 - val_loss: 0.2366 - val_accuracy: 0.6550\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 650us/step - loss: 0.2297 - accuracy: 0.6375 - val_loss: 0.2231 - val_accuracy: 0.6500\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 658us/step - loss: 0.2121 - accuracy: 0.6612 - val_loss: 0.2073 - val_accuracy: 0.6750\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 652us/step - loss: 0.1964 - accuracy: 0.6862 - val_loss: 0.2051 - val_accuracy: 0.6850\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 662us/step - loss: 0.1860 - accuracy: 0.7237 - val_loss: 0.1958 - val_accuracy: 0.7150\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 658us/step - loss: 0.1827 - accuracy: 0.7275 - val_loss: 0.2040 - val_accuracy: 0.6900\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 672us/step - loss: 0.1961 - accuracy: 0.6888 - val_loss: 0.1970 - val_accuracy: 0.6900\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 673us/step - loss: 0.1793 - accuracy: 0.7312 - val_loss: 0.1908 - val_accuracy: 0.6950\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 651us/step - loss: 0.1679 - accuracy: 0.7675 - val_loss: 0.1976 - val_accuracy: 0.7050\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.1636 - accuracy: 0.7650 - val_loss: 0.1950 - val_accuracy: 0.7200\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.1571 - accuracy: 0.7750 - val_loss: 0.1878 - val_accuracy: 0.7350\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.1517 - accuracy: 0.7950 - val_loss: 0.1868 - val_accuracy: 0.7100\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 681us/step - loss: 0.1400 - accuracy: 0.8012 - val_loss: 0.1925 - val_accuracy: 0.7200\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 763us/step - loss: 0.1358 - accuracy: 0.8112 - val_loss: 0.1841 - val_accuracy: 0.7150\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 663us/step - loss: 0.1349 - accuracy: 0.8100 - val_loss: 0.2317 - val_accuracy: 0.6900\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 663us/step - loss: 0.1309 - accuracy: 0.8325 - val_loss: 0.1942 - val_accuracy: 0.7100\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 660us/step - loss: 0.1254 - accuracy: 0.8300 - val_loss: 0.2049 - val_accuracy: 0.7300\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 662us/step - loss: 0.1155 - accuracy: 0.8425 - val_loss: 0.1993 - val_accuracy: 0.7200\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 656us/step - loss: 0.1040 - accuracy: 0.8712 - val_loss: 0.1968 - val_accuracy: 0.7000\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 662us/step - loss: 0.0981 - accuracy: 0.8763 - val_loss: 0.2167 - val_accuracy: 0.7200\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 654us/step - loss: 0.1012 - accuracy: 0.8725 - val_loss: 0.2071 - val_accuracy: 0.7200\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 660us/step - loss: 0.0953 - accuracy: 0.8838 - val_loss: 0.2000 - val_accuracy: 0.7050\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 666us/step - loss: 0.0845 - accuracy: 0.8975 - val_loss: 0.2092 - val_accuracy: 0.7000\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 665us/step - loss: 0.0766 - accuracy: 0.9112 - val_loss: 0.2081 - val_accuracy: 0.7300\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 646us/step - loss: 0.0749 - accuracy: 0.9112 - val_loss: 0.2110 - val_accuracy: 0.7250\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 671us/step - loss: 0.0686 - accuracy: 0.9275 - val_loss: 0.2416 - val_accuracy: 0.6800\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 664us/step - loss: 0.0731 - accuracy: 0.9200 - val_loss: 0.2170 - val_accuracy: 0.6900\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 653us/step - loss: 0.0664 - accuracy: 0.9300 - val_loss: 0.2104 - val_accuracy: 0.7200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 657us/step - loss: 0.0595 - accuracy: 0.9375 - val_loss: 0.2226 - val_accuracy: 0.6950\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 660us/step - loss: 0.0621 - accuracy: 0.9375 - val_loss: 0.2252 - val_accuracy: 0.6950\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 644us/step - loss: 0.0469 - accuracy: 0.9663 - val_loss: 0.2129 - val_accuracy: 0.7300\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 672us/step - loss: 0.0390 - accuracy: 0.9750 - val_loss: 0.2227 - val_accuracy: 0.7150\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 662us/step - loss: 0.0316 - accuracy: 0.9750 - val_loss: 0.2409 - val_accuracy: 0.6950\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 639us/step - loss: 0.0309 - accuracy: 0.9812 - val_loss: 0.2334 - val_accuracy: 0.7100\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 677us/step - loss: 0.0237 - accuracy: 0.9875 - val_loss: 0.2340 - val_accuracy: 0.7250\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 658us/step - loss: 0.0221 - accuracy: 0.9900 - val_loss: 0.2515 - val_accuracy: 0.7250\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 641us/step - loss: 0.0176 - accuracy: 0.9900 - val_loss: 0.2330 - val_accuracy: 0.7150\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 657us/step - loss: 0.0179 - accuracy: 0.9975 - val_loss: 0.2467 - val_accuracy: 0.7300\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 640us/step - loss: 0.0166 - accuracy: 0.9937 - val_loss: 0.2521 - val_accuracy: 0.7350\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 660us/step - loss: 0.0122 - accuracy: 0.9975 - val_loss: 0.2328 - val_accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 673us/step - loss: 0.0168 - accuracy: 0.9925 - val_loss: 0.2519 - val_accuracy: 0.7400\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 657us/step - loss: 0.0160 - accuracy: 0.9950 - val_loss: 0.2921 - val_accuracy: 0.7200\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 687us/step - loss: 0.0115 - accuracy: 0.9987 - val_loss: 0.2594 - val_accuracy: 0.7050\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 666us/step - loss: 0.0122 - accuracy: 0.9987 - val_loss: 0.3143 - val_accuracy: 0.7250\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 651us/step - loss: 0.0108 - accuracy: 0.9937 - val_loss: 0.2525 - val_accuracy: 0.7150\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 660us/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2500 - val_accuracy: 0.7400\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 662us/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.2388 - val_accuracy: 0.7400\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 650us/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.7350\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 661us/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2461 - val_accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_2.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_1 = model_2.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 1.0\n",
      "Train Set Accuracy: 0.7250000238418579\n",
      "processing time 29.868344699999994\n"
     ]
    }
   ],
   "source": [
    "accuracy_2 = model_2.evaluate(X1_test, y1_test, verbose = 0)\n",
    "accuracy_base_y_2 = model_2.evaluate(X1_train, y1_train, verbose = 0)\n",
    "stop_time_base_2 = time.clock()\n",
    "time_2 = stop_time_base_2 - start_time_base_2\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_2[1] )\n",
    "print(\"Train Set Accuracy:\", accuracy_2[1] )\n",
    "print(\"processing time\", time_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: RNN with LSTM cells and 5 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_3 = time.clock()\n",
    "def generateModel3():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 40, 32)            10624     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 43,937\n",
      "Trainable params: 43,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = generateModel3()\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.3090 - accuracy: 0.5038 - val_loss: 0.2543 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2536 - accuracy: 0.5100 - val_loss: 0.2616 - val_accuracy: 0.4700\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2461 - accuracy: 0.5800 - val_loss: 0.2411 - val_accuracy: 0.6450\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2376 - accuracy: 0.5975 - val_loss: 0.2277 - val_accuracy: 0.6400\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2265 - accuracy: 0.6363 - val_loss: 0.2238 - val_accuracy: 0.6500\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2184 - accuracy: 0.6438 - val_loss: 0.2171 - val_accuracy: 0.6750\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2111 - accuracy: 0.6737 - val_loss: 0.2167 - val_accuracy: 0.6650\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2107 - accuracy: 0.6600 - val_loss: 0.2147 - val_accuracy: 0.6750\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2052 - accuracy: 0.6700 - val_loss: 0.2127 - val_accuracy: 0.6800\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2010 - accuracy: 0.6775 - val_loss: 0.2115 - val_accuracy: 0.7000\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1976 - accuracy: 0.6888 - val_loss: 0.2087 - val_accuracy: 0.6850\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1897 - accuracy: 0.7038 - val_loss: 0.2075 - val_accuracy: 0.6800\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1905 - accuracy: 0.7050 - val_loss: 0.2092 - val_accuracy: 0.6750\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1855 - accuracy: 0.7100 - val_loss: 0.2165 - val_accuracy: 0.6650\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.7100 - val_loss: 0.2157 - val_accuracy: 0.6600\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1791 - accuracy: 0.7275 - val_loss: 0.2010 - val_accuracy: 0.7450\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1714 - accuracy: 0.7437 - val_loss: 0.2055 - val_accuracy: 0.7100\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1615 - accuracy: 0.7663 - val_loss: 0.2114 - val_accuracy: 0.7000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1629 - accuracy: 0.7513 - val_loss: 0.1946 - val_accuracy: 0.7100\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1564 - accuracy: 0.7800 - val_loss: 0.2019 - val_accuracy: 0.6950\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.7275 - val_loss: 0.2053 - val_accuracy: 0.6850\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1681 - accuracy: 0.7475 - val_loss: 0.2045 - val_accuracy: 0.7100\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1510 - accuracy: 0.7825 - val_loss: 0.2153 - val_accuracy: 0.6950\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1468 - accuracy: 0.7937 - val_loss: 0.1978 - val_accuracy: 0.7050\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1446 - accuracy: 0.7837 - val_loss: 0.2064 - val_accuracy: 0.6950\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1371 - accuracy: 0.8138 - val_loss: 0.2272 - val_accuracy: 0.6800\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1428 - accuracy: 0.7912 - val_loss: 0.2090 - val_accuracy: 0.6950\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1416 - accuracy: 0.7887 - val_loss: 0.2215 - val_accuracy: 0.7050\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1419 - accuracy: 0.7887 - val_loss: 0.2055 - val_accuracy: 0.6900\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1216 - accuracy: 0.8325 - val_loss: 0.2046 - val_accuracy: 0.7150\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1135 - accuracy: 0.8375 - val_loss: 0.2193 - val_accuracy: 0.7050\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1071 - accuracy: 0.8500 - val_loss: 0.2043 - val_accuracy: 0.7300\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1102 - accuracy: 0.8550 - val_loss: 0.2274 - val_accuracy: 0.7100\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0994 - accuracy: 0.8575 - val_loss: 0.2261 - val_accuracy: 0.7050\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0982 - accuracy: 0.8650 - val_loss: 0.2347 - val_accuracy: 0.7200\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0829 - accuracy: 0.9013 - val_loss: 0.2320 - val_accuracy: 0.7250\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0876 - accuracy: 0.8825 - val_loss: 0.2168 - val_accuracy: 0.7050\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0763 - accuracy: 0.9025 - val_loss: 0.2221 - val_accuracy: 0.7200\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0870 - accuracy: 0.8700 - val_loss: 0.2227 - val_accuracy: 0.6900\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0867 - accuracy: 0.8838 - val_loss: 0.2187 - val_accuracy: 0.7300\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0673 - accuracy: 0.9100 - val_loss: 0.2252 - val_accuracy: 0.7350\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0589 - accuracy: 0.9325 - val_loss: 0.2245 - val_accuracy: 0.7300\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0530 - accuracy: 0.9362 - val_loss: 0.2371 - val_accuracy: 0.7250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0570 - accuracy: 0.9250 - val_loss: 0.2562 - val_accuracy: 0.7200\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0611 - accuracy: 0.9237 - val_loss: 0.2416 - val_accuracy: 0.7000\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0498 - accuracy: 0.9438 - val_loss: 0.2552 - val_accuracy: 0.6900\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0516 - accuracy: 0.9400 - val_loss: 0.2383 - val_accuracy: 0.7350\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0493 - accuracy: 0.9325 - val_loss: 0.2476 - val_accuracy: 0.7100\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0614 - accuracy: 0.9137 - val_loss: 0.2682 - val_accuracy: 0.6950\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0458 - accuracy: 0.9400 - val_loss: 0.2353 - val_accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_3 = model_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7300000190734863\n",
      "Train Set Accuracy: 0.9737499952316284\n",
      "processing time 53.52331629999999\n"
     ]
    }
   ],
   "source": [
    "accuracy_3 = model_3.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy_base_y_3 = model_3.evaluate(X_train, y_train, verbose = 0)\n",
    "stop_time_base_3 = time.clock()\n",
    "time_3 = stop_time_base_3 - start_time_base_3\n",
    "print(\"Test Set Accuracy:\", accuracy_3[1])\n",
    "print(\"Train Set Accuracy:\", accuracy_base_y_3[1])\n",
    "print(\"processing time\", time_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4: RNN with LSTM cells and 5 Layers\n",
    "#### GloVe.6B.50d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_4 = time.clock()\n",
    "def generateModel4():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=LSTM(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 40, 32)            10624     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 43,937\n",
      "Trainable params: 43,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_4 = generateModel4()\n",
    "print(model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 3s 3ms/step - loss: 0.3457 - accuracy: 0.4875 - val_loss: 0.2491 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2606 - accuracy: 0.5138 - val_loss: 0.2579 - val_accuracy: 0.4700\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2435 - accuracy: 0.5788 - val_loss: 0.2394 - val_accuracy: 0.5800\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2297 - accuracy: 0.6350 - val_loss: 0.2261 - val_accuracy: 0.6400\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2144 - accuracy: 0.6413 - val_loss: 0.2353 - val_accuracy: 0.6200\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2158 - accuracy: 0.6575 - val_loss: 0.2201 - val_accuracy: 0.6600\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2080 - accuracy: 0.6775 - val_loss: 0.2148 - val_accuracy: 0.6850\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2037 - accuracy: 0.6762 - val_loss: 0.2116 - val_accuracy: 0.6750\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2006 - accuracy: 0.7088 - val_loss: 0.2061 - val_accuracy: 0.6850\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.7150 - val_loss: 0.2025 - val_accuracy: 0.7200\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1854 - accuracy: 0.7237 - val_loss: 0.1934 - val_accuracy: 0.7300\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1779 - accuracy: 0.7412 - val_loss: 0.1942 - val_accuracy: 0.7150\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1871 - accuracy: 0.7075 - val_loss: 0.1950 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.7387 - val_loss: 0.2053 - val_accuracy: 0.6650\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1824 - accuracy: 0.7262 - val_loss: 0.1871 - val_accuracy: 0.7450\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1680 - accuracy: 0.7538 - val_loss: 0.1859 - val_accuracy: 0.7250\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1713 - accuracy: 0.7513 - val_loss: 0.1850 - val_accuracy: 0.7300\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1692 - accuracy: 0.7525 - val_loss: 0.1904 - val_accuracy: 0.7050\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1532 - accuracy: 0.7775 - val_loss: 0.1850 - val_accuracy: 0.7250\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1518 - accuracy: 0.7738 - val_loss: 0.1862 - val_accuracy: 0.7200\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1438 - accuracy: 0.8000 - val_loss: 0.1878 - val_accuracy: 0.7300\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1384 - accuracy: 0.8037 - val_loss: 0.1813 - val_accuracy: 0.7450\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.8213 - val_loss: 0.1928 - val_accuracy: 0.7250\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1300 - accuracy: 0.8213 - val_loss: 0.1921 - val_accuracy: 0.7350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1155 - accuracy: 0.8500 - val_loss: 0.1866 - val_accuracy: 0.7300\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1050 - accuracy: 0.8712 - val_loss: 0.1932 - val_accuracy: 0.7350\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1008 - accuracy: 0.8687 - val_loss: 0.2017 - val_accuracy: 0.7100\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1001 - accuracy: 0.8650 - val_loss: 0.2052 - val_accuracy: 0.7550\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0869 - accuracy: 0.8975 - val_loss: 0.2077 - val_accuracy: 0.7300\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0787 - accuracy: 0.9025 - val_loss: 0.2134 - val_accuracy: 0.7350\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0827 - accuracy: 0.8875 - val_loss: 0.2157 - val_accuracy: 0.7200\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0857 - accuracy: 0.8838 - val_loss: 0.2059 - val_accuracy: 0.7500\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0674 - accuracy: 0.9050 - val_loss: 0.2170 - val_accuracy: 0.7300\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0521 - accuracy: 0.9275 - val_loss: 0.2224 - val_accuracy: 0.7400\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0501 - accuracy: 0.9362 - val_loss: 0.2144 - val_accuracy: 0.7200\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0427 - accuracy: 0.9463 - val_loss: 0.2226 - val_accuracy: 0.7100\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0425 - accuracy: 0.9500 - val_loss: 0.2366 - val_accuracy: 0.7200\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0428 - accuracy: 0.9463 - val_loss: 0.2334 - val_accuracy: 0.6950\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0359 - accuracy: 0.9588 - val_loss: 0.2269 - val_accuracy: 0.7250\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0321 - accuracy: 0.9613 - val_loss: 0.2225 - val_accuracy: 0.7350\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0261 - accuracy: 0.9712 - val_loss: 0.2485 - val_accuracy: 0.7400\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0212 - accuracy: 0.9762 - val_loss: 0.2662 - val_accuracy: 0.6550\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0220 - accuracy: 0.9762 - val_loss: 0.2410 - val_accuracy: 0.7250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0132 - accuracy: 0.9862 - val_loss: 0.2492 - val_accuracy: 0.6900\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0118 - accuracy: 0.9900 - val_loss: 0.2370 - val_accuracy: 0.7400\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.2369 - val_accuracy: 0.7250\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.2359 - val_accuracy: 0.7250\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.2303 - val_accuracy: 0.7150\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9987 - val_loss: 0.2339 - val_accuracy: 0.7150\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0024 - accuracy: 0.9987 - val_loss: 0.2389 - val_accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "model_4.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_4 = model_4.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.9987499713897705\n",
      "Train Set Accuracy: 0.7099999785423279\n",
      "processing time 57.5640368\n"
     ]
    }
   ],
   "source": [
    "accuracy_4 = model_4.evaluate(X1_test, y1_test, verbose = 0)\n",
    "accuracy_base_y_4 = model_4.evaluate(X1_train, y1_train, verbose = 0)\n",
    "stop_time_base_4 = time.clock()\n",
    "time_4 = stop_time_base_4 - start_time_base_4\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_4[1])\n",
    "print(\"Train Set Accuracy:\", accuracy_4[1])\n",
    "print(\"processing time\", time_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5: RNN with GRU cells and 3 Layers\n",
    "#### GloVe.6B.100d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU\n",
    "start_time_base_5 = time.clock()\n",
    "def generateModel5():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 40, 32)            7968      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_5 = generateModel5()\n",
    "print(model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.5000 - val_loss: 0.2571 - val_accuracy: 0.5200\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 794us/step - loss: 0.2448 - accuracy: 0.5675 - val_loss: 0.2443 - val_accuracy: 0.6000\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2370 - accuracy: 0.6025 - val_loss: 0.2418 - val_accuracy: 0.5700\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2285 - accuracy: 0.6150 - val_loss: 0.2325 - val_accuracy: 0.6350\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 908us/step - loss: 0.2248 - accuracy: 0.6237 - val_loss: 0.2347 - val_accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2149 - accuracy: 0.6612 - val_loss: 0.2223 - val_accuracy: 0.6400\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2065 - accuracy: 0.6725 - val_loss: 0.2199 - val_accuracy: 0.6700\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2015 - accuracy: 0.6825 - val_loss: 0.2126 - val_accuracy: 0.6400\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 894us/step - loss: 0.1960 - accuracy: 0.6988 - val_loss: 0.2289 - val_accuracy: 0.6400\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 984us/step - loss: 0.1942 - accuracy: 0.7000 - val_loss: 0.2079 - val_accuracy: 0.6550\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 977us/step - loss: 0.1819 - accuracy: 0.7287 - val_loss: 0.2056 - val_accuracy: 0.6900\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1749 - accuracy: 0.7412 - val_loss: 0.1995 - val_accuracy: 0.6850\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1695 - accuracy: 0.7475 - val_loss: 0.2044 - val_accuracy: 0.6950\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 892us/step - loss: 0.1660 - accuracy: 0.7563 - val_loss: 0.2067 - val_accuracy: 0.6850\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1702 - accuracy: 0.7525 - val_loss: 0.1977 - val_accuracy: 0.6850\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1638 - accuracy: 0.7650 - val_loss: 0.2309 - val_accuracy: 0.6300\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 962us/step - loss: 0.1578 - accuracy: 0.7738 - val_loss: 0.2128 - val_accuracy: 0.6750\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 942us/step - loss: 0.1683 - accuracy: 0.7588 - val_loss: 0.2005 - val_accuracy: 0.7100\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 922us/step - loss: 0.1580 - accuracy: 0.7725 - val_loss: 0.2075 - val_accuracy: 0.7300\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1417 - accuracy: 0.8025 - val_loss: 0.1998 - val_accuracy: 0.7100\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 996us/step - loss: 0.1431 - accuracy: 0.7900 - val_loss: 0.2228 - val_accuracy: 0.7250\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1361 - accuracy: 0.8200 - val_loss: 0.1992 - val_accuracy: 0.7200\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1332 - accuracy: 0.8300 - val_loss: 0.1981 - val_accuracy: 0.6950\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 846us/step - loss: 0.1206 - accuracy: 0.8500 - val_loss: 0.1998 - val_accuracy: 0.7100\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 852us/step - loss: 0.1180 - accuracy: 0.8575 - val_loss: 0.2024 - val_accuracy: 0.6900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 832us/step - loss: 0.1158 - accuracy: 0.8450 - val_loss: 0.2005 - val_accuracy: 0.6950\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 824us/step - loss: 0.1057 - accuracy: 0.8700 - val_loss: 0.2092 - val_accuracy: 0.7000\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 791us/step - loss: 0.1041 - accuracy: 0.8700 - val_loss: 0.2100 - val_accuracy: 0.7050\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 784us/step - loss: 0.1058 - accuracy: 0.8750 - val_loss: 0.2136 - val_accuracy: 0.7100\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 806us/step - loss: 0.1049 - accuracy: 0.8763 - val_loss: 0.2097 - val_accuracy: 0.7050\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 900us/step - loss: 0.0920 - accuracy: 0.9050 - val_loss: 0.2199 - val_accuracy: 0.6750\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 855us/step - loss: 0.0924 - accuracy: 0.8913 - val_loss: 0.2305 - val_accuracy: 0.7100\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 952us/step - loss: 0.0947 - accuracy: 0.8988 - val_loss: 0.2605 - val_accuracy: 0.6500\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0927 - accuracy: 0.9000 - val_loss: 0.2140 - val_accuracy: 0.7050\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 922us/step - loss: 0.0780 - accuracy: 0.9237 - val_loss: 0.2229 - val_accuracy: 0.7050\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 906us/step - loss: 0.0740 - accuracy: 0.9275 - val_loss: 0.2266 - val_accuracy: 0.6950\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 933us/step - loss: 0.0729 - accuracy: 0.9350 - val_loss: 0.2237 - val_accuracy: 0.6950\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 847us/step - loss: 0.0682 - accuracy: 0.9388 - val_loss: 0.2574 - val_accuracy: 0.6650\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 791us/step - loss: 0.0671 - accuracy: 0.9463 - val_loss: 0.2312 - val_accuracy: 0.6950\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 944us/step - loss: 0.0623 - accuracy: 0.9450 - val_loss: 0.2365 - val_accuracy: 0.6900\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 899us/step - loss: 0.0567 - accuracy: 0.9500 - val_loss: 0.2366 - val_accuracy: 0.7050\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 827us/step - loss: 0.0592 - accuracy: 0.9575 - val_loss: 0.2447 - val_accuracy: 0.6950\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 832us/step - loss: 0.0546 - accuracy: 0.9600 - val_loss: 0.2428 - val_accuracy: 0.7000\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 917us/step - loss: 0.0514 - accuracy: 0.9712 - val_loss: 0.2512 - val_accuracy: 0.6850\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 989us/step - loss: 0.0458 - accuracy: 0.9762 - val_loss: 0.2468 - val_accuracy: 0.6900\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 899us/step - loss: 0.0456 - accuracy: 0.9762 - val_loss: 0.2568 - val_accuracy: 0.6950\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 830us/step - loss: 0.0410 - accuracy: 0.9825 - val_loss: 0.2504 - val_accuracy: 0.6850\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 757us/step - loss: 0.0423 - accuracy: 0.9800 - val_loss: 0.2596 - val_accuracy: 0.7100\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 759us/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.2506 - val_accuracy: 0.6950\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 821us/step - loss: 0.0373 - accuracy: 0.9900 - val_loss: 0.2533 - val_accuracy: 0.7100\n"
     ]
    }
   ],
   "source": [
    "model_5.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_5 = model_5.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 306us/step\n",
      "Test Set Accuracy: 0.9912499785423279\n",
      "Train Set Accuracy: 0.7099999785423279\n",
      "processing time 41.97401099999999\n"
     ]
    }
   ],
   "source": [
    "accuracy_5 = model_5.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy_base_y_5 = model_5.evaluate(X_train, y_train)\n",
    "stop_time_base_5 = time.clock()\n",
    "time_5 = stop_time_base_5 - start_time_base_5\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_5[1] )\n",
    "print(\"Train Set Accuracy:\", accuracy_5[1] )\n",
    "print(\"processing time\", time_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6: RNN with GRU cells and 3 Layers\n",
    "#### GloVe.6B.100d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_6 = time.clock()\n",
    "def generateModel6():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 40, 32)            7968      \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 20,481\n",
      "Trainable params: 20,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_6 = generateModel6()\n",
    "print(model_6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2599 - accuracy: 0.5050 - val_loss: 0.2485 - val_accuracy: 0.5200\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2405 - accuracy: 0.5788 - val_loss: 0.2397 - val_accuracy: 0.5750\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2295 - accuracy: 0.6062 - val_loss: 0.2533 - val_accuracy: 0.5250\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2247 - accuracy: 0.6250 - val_loss: 0.2247 - val_accuracy: 0.6100\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2114 - accuracy: 0.6687 - val_loss: 0.2168 - val_accuracy: 0.6400\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2067 - accuracy: 0.6875 - val_loss: 0.2151 - val_accuracy: 0.6650\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2000 - accuracy: 0.6913 - val_loss: 0.2084 - val_accuracy: 0.6800\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1896 - accuracy: 0.7250 - val_loss: 0.2041 - val_accuracy: 0.6650\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 957us/step - loss: 0.1829 - accuracy: 0.7412 - val_loss: 0.1996 - val_accuracy: 0.6700\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 968us/step - loss: 0.1811 - accuracy: 0.7387 - val_loss: 0.2215 - val_accuracy: 0.6650\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1766 - accuracy: 0.7575 - val_loss: 0.2029 - val_accuracy: 0.6950\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1619 - accuracy: 0.7750 - val_loss: 0.2026 - val_accuracy: 0.6900\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1564 - accuracy: 0.7788 - val_loss: 0.2164 - val_accuracy: 0.7050\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1556 - accuracy: 0.7862 - val_loss: 0.2019 - val_accuracy: 0.6750\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1513 - accuracy: 0.8075 - val_loss: 0.1956 - val_accuracy: 0.6750\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1386 - accuracy: 0.8175 - val_loss: 0.2003 - val_accuracy: 0.6800\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1341 - accuracy: 0.8300 - val_loss: 0.1980 - val_accuracy: 0.7000\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.8450 - val_loss: 0.2012 - val_accuracy: 0.7150\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.8475 - val_loss: 0.2186 - val_accuracy: 0.6950\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.8462 - val_loss: 0.2047 - val_accuracy: 0.7050\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 998us/step - loss: 0.1103 - accuracy: 0.8750 - val_loss: 0.2045 - val_accuracy: 0.7100\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.8825 - val_loss: 0.2070 - val_accuracy: 0.7250\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1037 - accuracy: 0.8750 - val_loss: 0.2086 - val_accuracy: 0.7350\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0978 - accuracy: 0.8913 - val_loss: 0.2150 - val_accuracy: 0.7350\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1006 - accuracy: 0.8825 - val_loss: 0.2131 - val_accuracy: 0.6900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0913 - accuracy: 0.9137 - val_loss: 0.2115 - val_accuracy: 0.7100\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0953 - accuracy: 0.8975 - val_loss: 0.2091 - val_accuracy: 0.7350\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0841 - accuracy: 0.9200 - val_loss: 0.2108 - val_accuracy: 0.7000\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0768 - accuracy: 0.9250 - val_loss: 0.2211 - val_accuracy: 0.7150\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0724 - accuracy: 0.9450 - val_loss: 0.2103 - val_accuracy: 0.7200\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0710 - accuracy: 0.9438 - val_loss: 0.2378 - val_accuracy: 0.6950\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0688 - accuracy: 0.9400 - val_loss: 0.2158 - val_accuracy: 0.7300\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0619 - accuracy: 0.9563 - val_loss: 0.2204 - val_accuracy: 0.7050\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0584 - accuracy: 0.9625 - val_loss: 0.2171 - val_accuracy: 0.7000\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0549 - accuracy: 0.9663 - val_loss: 0.2250 - val_accuracy: 0.6900\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0519 - accuracy: 0.9750 - val_loss: 0.2374 - val_accuracy: 0.7100\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 978us/step - loss: 0.0571 - accuracy: 0.9625 - val_loss: 0.2195 - val_accuracy: 0.6900\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0505 - accuracy: 0.9700 - val_loss: 0.2281 - val_accuracy: 0.6750\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 999us/step - loss: 0.0449 - accuracy: 0.9825 - val_loss: 0.2154 - val_accuracy: 0.7150\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 843us/step - loss: 0.0419 - accuracy: 0.9837 - val_loss: 0.2229 - val_accuracy: 0.6900\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 932us/step - loss: 0.0382 - accuracy: 0.9875 - val_loss: 0.2195 - val_accuracy: 0.6900\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0392 - accuracy: 0.9862 - val_loss: 0.2168 - val_accuracy: 0.7050\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 938us/step - loss: 0.0335 - accuracy: 0.9912 - val_loss: 0.2360 - val_accuracy: 0.7100\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 879us/step - loss: 0.0384 - accuracy: 0.9912 - val_loss: 0.2211 - val_accuracy: 0.6950\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 789us/step - loss: 0.0342 - accuracy: 0.9900 - val_loss: 0.2214 - val_accuracy: 0.6900\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 754us/step - loss: 0.0283 - accuracy: 0.9925 - val_loss: 0.2165 - val_accuracy: 0.7050\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 841us/step - loss: 0.0251 - accuracy: 0.9950 - val_loss: 0.2261 - val_accuracy: 0.7000\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0228 - accuracy: 0.9962 - val_loss: 0.2206 - val_accuracy: 0.6900\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0221 - accuracy: 0.9962 - val_loss: 0.2239 - val_accuracy: 0.6950\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 823us/step - loss: 0.0197 - accuracy: 0.9962 - val_loss: 0.2211 - val_accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "model_6.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_6 = model_6.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 309us/step\n",
      "Test Set Accuracy: 0.9975000023841858\n",
      "Train Set Accuracy: 0.6850000023841858\n",
      "processing time 49.41685900000002\n"
     ]
    }
   ],
   "source": [
    "accuracy_6 = model_6.evaluate(X1_test, y1_test, verbose = 0)\n",
    "accuracy_base_y_6 = model_6.evaluate(X1_train, y1_train)\n",
    "stop_time_base_6 = time.clock()\n",
    "time_6 = stop_time_base_6 - start_time_base_6\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_6[1] )\n",
    "print(\"Train Set Accuracy:\",accuracy_6[1] )\n",
    "print(\"processing time\", time_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7: RNN with GRU cells and 5 Layers\n",
    "#### GloVe.6B.100d - Embedding Vocabulary Size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_7 = time.clock()\n",
    "def generateModel7():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 40, 32)            7968      \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 32,961\n",
      "Trainable params: 32,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_7 = generateModel7()\n",
    "print(model_7.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3774 - accuracy: 0.4638 - val_loss: 0.2497 - val_accuracy: 0.5500\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2545 - accuracy: 0.5050 - val_loss: 0.2510 - val_accuracy: 0.4900\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2442 - accuracy: 0.5650 - val_loss: 0.2420 - val_accuracy: 0.5750\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2394 - accuracy: 0.5962 - val_loss: 0.2379 - val_accuracy: 0.6050\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2328 - accuracy: 0.6112 - val_loss: 0.2371 - val_accuracy: 0.5750\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.2274 - accuracy: 0.6150 - val_loss: 0.2554 - val_accuracy: 0.5650\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2219 - accuracy: 0.6375 - val_loss: 0.2459 - val_accuracy: 0.5750\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.6475 - val_loss: 0.2197 - val_accuracy: 0.6550\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.2049 - accuracy: 0.6800 - val_loss: 0.2260 - val_accuracy: 0.6450\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.1973 - accuracy: 0.6938 - val_loss: 0.2463 - val_accuracy: 0.5900\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.7063 - val_loss: 0.2036 - val_accuracy: 0.6850\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.7250 - val_loss: 0.2068 - val_accuracy: 0.7050\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.7350 - val_loss: 0.2005 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.7337 - val_loss: 0.2044 - val_accuracy: 0.6850\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.7300 - val_loss: 0.2106 - val_accuracy: 0.6750\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1696 - accuracy: 0.7563 - val_loss: 0.2020 - val_accuracy: 0.6900\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1604 - accuracy: 0.7575 - val_loss: 0.2058 - val_accuracy: 0.6900\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1532 - accuracy: 0.7738 - val_loss: 0.2068 - val_accuracy: 0.6900\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1531 - accuracy: 0.7675 - val_loss: 0.2080 - val_accuracy: 0.7200\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1492 - accuracy: 0.7750 - val_loss: 0.2058 - val_accuracy: 0.7000\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1486 - accuracy: 0.7925 - val_loss: 0.2034 - val_accuracy: 0.7100\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1398 - accuracy: 0.7987 - val_loss: 0.2126 - val_accuracy: 0.7050\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1339 - accuracy: 0.8125 - val_loss: 0.2119 - val_accuracy: 0.6950\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.8300 - val_loss: 0.2155 - val_accuracy: 0.7100\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1226 - accuracy: 0.8400 - val_loss: 0.2139 - val_accuracy: 0.7200\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.8575 - val_loss: 0.2386 - val_accuracy: 0.7000\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.8687 - val_loss: 0.2202 - val_accuracy: 0.7100\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1037 - accuracy: 0.8850 - val_loss: 0.2450 - val_accuracy: 0.7100\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0981 - accuracy: 0.8838 - val_loss: 0.2385 - val_accuracy: 0.7200\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0937 - accuracy: 0.8975 - val_loss: 0.2655 - val_accuracy: 0.6500\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1045 - accuracy: 0.8737 - val_loss: 0.2490 - val_accuracy: 0.6900\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0885 - accuracy: 0.9125 - val_loss: 0.2457 - val_accuracy: 0.7150\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9038 - val_loss: 0.2595 - val_accuracy: 0.6850\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0767 - accuracy: 0.9225 - val_loss: 0.2571 - val_accuracy: 0.7100\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0676 - accuracy: 0.9400 - val_loss: 0.2609 - val_accuracy: 0.6950\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0693 - accuracy: 0.9375 - val_loss: 0.2742 - val_accuracy: 0.6950\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0588 - accuracy: 0.9500 - val_loss: 0.2668 - val_accuracy: 0.7000\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0534 - accuracy: 0.9650 - val_loss: 0.2705 - val_accuracy: 0.6950\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0524 - accuracy: 0.9762 - val_loss: 0.2813 - val_accuracy: 0.7050\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0496 - accuracy: 0.9725 - val_loss: 0.2819 - val_accuracy: 0.6850\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0446 - accuracy: 0.9762 - val_loss: 0.2738 - val_accuracy: 0.7000\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0435 - accuracy: 0.9750 - val_loss: 0.2943 - val_accuracy: 0.6650\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0379 - accuracy: 0.9825 - val_loss: 0.2810 - val_accuracy: 0.6950\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0333 - accuracy: 0.9862 - val_loss: 0.2862 - val_accuracy: 0.6750\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0328 - accuracy: 0.9850 - val_loss: 0.2884 - val_accuracy: 0.6850\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0267 - accuracy: 0.9900 - val_loss: 0.2879 - val_accuracy: 0.7000\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0230 - accuracy: 0.9925 - val_loss: 0.2773 - val_accuracy: 0.6900\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0205 - accuracy: 0.9925 - val_loss: 0.2777 - val_accuracy: 0.6850\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0190 - accuracy: 0.9975 - val_loss: 0.2845 - val_accuracy: 0.6950\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0257 - accuracy: 0.9862 - val_loss: 0.2937 - val_accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "model_7.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_7 = model_7.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 553us/step\n",
      "Test Set Accuracy: 0.9987499713897705\n",
      "Train Set Accuracy: 0.6850000023841858\n",
      "processing time 75.27545650000002\n"
     ]
    }
   ],
   "source": [
    "accuracy_7 = model_7.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy_base_y_7 = model_7.evaluate(X_train, y_train)\n",
    "stop_time_base_7 = time.clock()\n",
    "time_7 = stop_time_base_7 - start_time_base_7\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_7[1])\n",
    "print(\"Train Set Accuracy:\",accuracy_7[1] )\n",
    "print(\"processing time\", time_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8: RNN with GRU cells and 5 Layers\n",
    "#### GloVe.6B.100d - Embedding Vocabulary Size 30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_base_8 = time.clock()\n",
    "def generateModel8():\n",
    "    inputLayer=Input(shape=(40,50))\n",
    "    unitNum = 32\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(inputLayer)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, return_sequences=True, activation='elu')(model)\n",
    "    model=GRU(units=unitNum, activation='elu')(model)\n",
    "    outputModel=Dense(1)(model)\n",
    "    model=Model(inputLayer,outputModel)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 40, 32)            7968      \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 40, 32)            6240      \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 32,961\n",
      "Trainable params: 32,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_8 = generateModel8()\n",
    "print(model_8.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.2820 - accuracy: 0.5138 - val_loss: 0.2551 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2546 - accuracy: 0.5138 - val_loss: 0.2467 - val_accuracy: 0.5350\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2392 - accuracy: 0.6112 - val_loss: 0.2358 - val_accuracy: 0.5750\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2375 - accuracy: 0.5987 - val_loss: 0.2303 - val_accuracy: 0.6050\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2254 - accuracy: 0.6413 - val_loss: 0.2208 - val_accuracy: 0.6400\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2126 - accuracy: 0.6737 - val_loss: 0.2105 - val_accuracy: 0.6850\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.2028 - accuracy: 0.6975 - val_loss: 0.2030 - val_accuracy: 0.6950\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.7050 - val_loss: 0.2059 - val_accuracy: 0.6500\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.7138 - val_loss: 0.2116 - val_accuracy: 0.6850\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1819 - accuracy: 0.7350 - val_loss: 0.1980 - val_accuracy: 0.6950\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.7487 - val_loss: 0.2174 - val_accuracy: 0.6650\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1658 - accuracy: 0.7638 - val_loss: 0.1947 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1559 - accuracy: 0.7800 - val_loss: 0.1912 - val_accuracy: 0.7000\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1492 - accuracy: 0.8012 - val_loss: 0.2089 - val_accuracy: 0.7000\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.1487 - accuracy: 0.7887 - val_loss: 0.1867 - val_accuracy: 0.6950\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1371 - accuracy: 0.8175 - val_loss: 0.1952 - val_accuracy: 0.6850\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1321 - accuracy: 0.8413 - val_loss: 0.1947 - val_accuracy: 0.7050\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.8438 - val_loss: 0.1926 - val_accuracy: 0.7000\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.8625 - val_loss: 0.1912 - val_accuracy: 0.7150\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1129 - accuracy: 0.8650 - val_loss: 0.2296 - val_accuracy: 0.6900\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.1163 - accuracy: 0.8550 - val_loss: 0.2033 - val_accuracy: 0.7150\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0969 - accuracy: 0.8925 - val_loss: 0.2127 - val_accuracy: 0.7000\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0971 - accuracy: 0.8963 - val_loss: 0.2199 - val_accuracy: 0.6950\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0921 - accuracy: 0.9038 - val_loss: 0.2021 - val_accuracy: 0.7200\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9175 - val_loss: 0.2123 - val_accuracy: 0.7000\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0818 - accuracy: 0.9175 - val_loss: 0.2350 - val_accuracy: 0.6750\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.9212 - val_loss: 0.2376 - val_accuracy: 0.6800\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.9375 - val_loss: 0.2167 - val_accuracy: 0.7250\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0625 - accuracy: 0.9450 - val_loss: 0.2353 - val_accuracy: 0.7000\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0580 - accuracy: 0.9600 - val_loss: 0.2459 - val_accuracy: 0.6750\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0534 - accuracy: 0.9600 - val_loss: 0.2205 - val_accuracy: 0.7250\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0473 - accuracy: 0.9700 - val_loss: 0.2211 - val_accuracy: 0.7300\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0436 - accuracy: 0.9800 - val_loss: 0.2322 - val_accuracy: 0.7200\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 2s 2ms/step - loss: 0.0459 - accuracy: 0.9625 - val_loss: 0.2372 - val_accuracy: 0.7200\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0402 - accuracy: 0.9675 - val_loss: 0.2318 - val_accuracy: 0.7200\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0326 - accuracy: 0.9800 - val_loss: 0.2389 - val_accuracy: 0.7200\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9850 - val_loss: 0.2407 - val_accuracy: 0.7600\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0274 - accuracy: 0.9837 - val_loss: 0.2398 - val_accuracy: 0.7000\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0222 - accuracy: 0.9887 - val_loss: 0.2322 - val_accuracy: 0.7150\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0193 - accuracy: 0.9900 - val_loss: 0.2243 - val_accuracy: 0.7300\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0171 - accuracy: 0.9887 - val_loss: 0.2378 - val_accuracy: 0.7250\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0150 - accuracy: 0.9887 - val_loss: 0.2302 - val_accuracy: 0.7250\n",
      "Epoch 43/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9925 - val_loss: 0.2261 - val_accuracy: 0.7250\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0111 - accuracy: 0.9937 - val_loss: 0.2217 - val_accuracy: 0.7200\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0097 - accuracy: 0.9962 - val_loss: 0.2300 - val_accuracy: 0.7250\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0094 - accuracy: 0.9962 - val_loss: 0.2217 - val_accuracy: 0.7350\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0074 - accuracy: 0.9962 - val_loss: 0.2224 - val_accuracy: 0.7300\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.2242 - val_accuracy: 0.7300\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.2243 - val_accuracy: 0.7400\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.2145 - val_accuracy: 0.7450\n"
     ]
    }
   ],
   "source": [
    "model_8.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "lossThreshold = .1\n",
    "valLossThreshold = .1\n",
    "\n",
    "hist_8 = model_8.fit(X1_train, y1_train, validation_data=(X1_test, y1_test), epochs=50, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 1s 728us/step\n",
      "Test Set Accuracy: 0.9987499713897705\n",
      "Train Set Accuracy: 0.7450000047683716\n",
      "processing time 73.20194680000003\n"
     ]
    }
   ],
   "source": [
    "accuracy_8 = model_8.evaluate(X1_test, y1_test, verbose = 0)\n",
    "accuracy_base_y_8 = model_8.evaluate(X1_train, y1_train)\n",
    "stop_time_base_8 = time.clock()\n",
    "time_8 = stop_time_base_8 - start_time_base_8\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_base_y_8[1])\n",
    "print(\"Train Set Accuracy:\",accuracy_8[1] )\n",
    "print(\"processing time\", time_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Summary Table -------------------------------\n",
      "# - Model                 Number of Layers    Embedding Vocabulary Size    Processing Time    Test Set Accuracy    Training Set Accuracy\n",
      "------------------------  ------------------  ---------------------------  -----------------  -------------------  -----------------------\n",
      "1 - Simple RNN with BPTT  3                   10000                        18.105             0.68                 0.77625\n",
      "2 - RNN with LSTM cells   3                   10000                        31.7837            0.685                0.9975\n",
      "3 - RNN with LSTM cells   3                   30000                        29.8683            0.725                1\n",
      "4 - RNN with LSTM cells   5                   10000                        53.5233            0.73                 0.97375\n",
      "5 - RNN with LSTM cells   5                   30000                        57.564             0.71                 0.99875\n",
      "6 - RNN with GRU          3                   10000                        41.974             0.71                 0.99125\n",
      "7 - RNN with GRU          3                   30000                        49.4169            0.685                0.9975\n",
      "8 - RNN with GRU          5                   10000                        75.2755            0.685                0.99875\n",
      "9 - RNN with GRU          5                   30000                        73.2019            0.745                0.99875\n"
     ]
    }
   ],
   "source": [
    "#### Summary Table\n",
    "from tabulate import tabulate\n",
    "\n",
    "col_labels = ['# - Model', 'Number of Layers', 'Embedding Vocabulary Size','Processing Time', 'Test Set Accuracy', 'Training Set Accuracy']\n",
    "\n",
    "table_vals = [['1 - Simple RNN with BPTT','3','10000',runtime_base, accuracy_base,accuracy_base_y],\n",
    "              ['2 - RNN with LSTM cells','3','10000',time_1,accuracy_1[1],accuracy_base_y_1[1]],\n",
    "              ['3 - RNN with LSTM cells','3','30000',time_2,accuracy_2[1],accuracy_base_y_2[1]],\n",
    "              ['4 - RNN with LSTM cells','5','10000',time_3,accuracy_3[1],accuracy_base_y_3[1]],\n",
    "              ['5 - RNN with LSTM cells','5','30000',time_4,accuracy_4[1],accuracy_base_y_4[1]],\n",
    "              ['6 - RNN with GRU','3','10000',time_5,accuracy_5[1],accuracy_base_y_5[1]],\n",
    "              ['7 - RNN with GRU','3','30000',time_6,accuracy_6[1],accuracy_base_y_6[1]],\n",
    "              ['8 - RNN with GRU','5','10000',time_7,accuracy_7[1],accuracy_base_y_7[1]],\n",
    "              ['9 - RNN with GRU','5','30000',time_8,accuracy_8[1],accuracy_base_y_8[1]],]\n",
    "\n",
    "print('------------------------------- Summary Table -------------------------------')\n",
    "\n",
    "table = tabulate(table_vals, headers=col_labels, tablefmt=\"simple\",numalign=\"left\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Management Problem\n",
    "- Suppose management is thinking about using a language model to classify written customer reviews and call and complaint logs. If the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers.\n",
    "- How would you advise senior management? What kinds of systems and methods would be most relevant to the customer services function? Considering the results of this assignment in particular, what is needed to make an automated customer support system that is capable of identifying negative customer feelings? What can data scientists do to make language models more useful in a customer service function?\n",
    "\n",
    "\n",
    "#### REPORT/FINDINGS: \n",
    "(1) A summary and problem definition for management; \n",
    "\n",
    "    For this week assigment, the project mainly focus on building RNN model to analysis the movie reviews and classify written customer reviews. The purpose of this project is to define the most critical customer messages, then customer support can contact those customers.\n",
    "\n",
    "(2) Discussion of the research design, measurement and statistical methods, traditional and machine learning methods employed \n",
    "    \n",
    "    I have created 9 models including the simple RNN with BPTT, moded # 2-5 are LSTM cells, and 6-9 are GRU. Overall compare with LSTM and GRU, GRU takes more processing time. but under the same layers, Number of layers and embedding vocabulary size, GRU has more accurate accuracy for both training set and test set. Within LSTM, the layers and more embedding vocabulary size will take more processing time. but when the embedding vocabulary size is 30000, the more number of layers has less accuracy for both training set and test. With in GRU model, when the embedding vocabulary size is 10000, the more number of layers have less training and test accuracy. \n",
    "    \n",
    "(3) Overview of programming work; \n",
    "\n",
    "    first, to install chakin package to get embeddings, then set embedding size, the most important part is to convert text to numpy array and set negative review to 0, and positive reviw to 1. split training data to 80% and 20%. and create LSTM and GRU with different layers and embedding vocabulary size.\n",
    "    \n",
    "(4) Review of results with recommendations for management.\n",
    "\n",
    "    after comparing with all the models I create, I think the GRU with 5 layers and 30000 embedding vocabulary size has the accuracy. I think the system can ask customer rate the moviews and write reviews. For example, if customer leave a 1 star feedback, the system can identify the negative custmer. \n",
    "    Also the system need to create target customer personas, then understand and create emotional connections, anticipate customer needs and be proactive, the collect feedback and perform analytics.\n",
    "    \n",
    "    resource: https://www.proprofs.com/c/customer-support/emotions-impact-customer-engagement/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
